{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pads_Unks (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q4KDFjVQqX8t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "zHL5x_AkqnIZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxYEElOwq8rb",
        "outputId": "dd527320-dc21-49e7-c41e-e452dff1d214"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.13)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.1.0/ru_core_news_sm-3.1.0.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aIQ8gnb9rgBQ",
        "outputId": "82cf377e-2cfb-4ff3-bee3-1db008e85ac6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.1.0/ru_core_news_sm-3.1.0.tar.gz\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.1.0/ru_core_news_sm-3.1.0.tar.gz (15.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.9 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting spacy<3.2.0,>=3.1.0\n",
            "  Downloading spacy-3.1.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting pymorphy2>=0.9\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2>=0.9->ru-core-news-sm==3.1.0) (0.6.2)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 34.1 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (0.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (2.4.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (0.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (3.10.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (4.62.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (8.0.13)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (2021.10.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->ru-core-news-sm==3.1.0) (2.0.1)\n",
            "Building wheels for collected packages: ru-core-news-sm\n",
            "  Building wheel for ru-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ru-core-news-sm: filename=ru_core_news_sm-3.1.0-py3-none-any.whl size=16086411 sha256=cfadaf86db7faa803611a63e8c75d8f15f3149d75cf3cacb10f033a8d89b7b96\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/a7/44/ee07931d9211e738e3b23f0321c9ff7a7ee2ffcad9946a9e07\n",
            "Successfully built ru-core-news-sm\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, spacy, pymorphy2, ru-core-news-sm\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.2.1\n",
            "    Uninstalling spacy-3.2.1:\n",
            "      Successfully uninstalled spacy-3.2.1\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 ru-core-news-sm-3.1.0 spacy-3.1.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "spacy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_ru = spacy.load('ru_core_news_sm')"
      ],
      "metadata": {
        "id": "WxM9ilR5quWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f673b3d-f3e0-4c29-bc78-cbce54f064c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/util.py:833: UserWarning: [W095] Model 'ru_core_news_sm' (3.1.0) was trained with spaCy v3.1 and may not be 100% compatible with the current version (3.2.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_ru(text):\n",
        "  return [tok.text for tok in spacy_ru.tokenizer(text)]"
      ],
      "metadata": {
        "id": "9_IpzHG2rwJ4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('questions_small.txt') as f:\n",
        "        questions = f.read().splitlines()\n",
        "with open('answers_small.txt') as f:\n",
        "        answers = f.read().splitlines()"
      ],
      "metadata": {
        "id": "LG95sYlbsAqC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_q(text):\n",
        "  return [tok.text for tok in questions.tokenizer(text)]\n",
        "def tokenize_a(text):\n",
        "  return [tok.text for tok in answers.tokenizer(text)]"
      ],
      "metadata": {
        "id": "upOepVTNsZha"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRC = Field(tokenize = tokenize_q, \n",
        "            init_token = '[BOS]', \n",
        "            eos_token = '[EOS]', \n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_a, \n",
        "            init_token = '[BOS]', \n",
        "            eos_token = '[EOS]', \n",
        "            lower = True)"
      ],
      "metadata": {
        "id": "oA4zH0sbskWp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLd9P31Ot6Ou",
        "outputId": "3e445bc3-a704-4c16-861b-17b617b87fb4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.tokenize_q>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext import datasets, data\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "vu8b2O7us35a"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUe_DNJ4KSkh",
        "outputId": "3d42454e-b2b1-4d37-bac6-2eed4da9bafa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.11.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer"
      ],
      "metadata": {
        "id": "ApifpWchJGoX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(BPE())\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = BpeTrainer(special_tokens=[ '[EOS]', '[BOS]', '[PAD]'])\n",
        "tokenizer.train(files=[\"questions_small.txt\", \"answers_small.txt\"], trainer=trainer)"
      ],
      "metadata": {
        "id": "S5PEW4nCJDQN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy import data"
      ],
      "metadata": {
        "id": "jtyV9Dy3NAYx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='[BOS]',\n",
        "    eos_token='[EOS]',\n",
        "    pad_token='[PAD]',\n",
        "    lower=True,\n",
        "    tokenize=lambda x: tokenizer.encode(x).tokens,\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "fields = (('src', TEXT), ('tgt', TEXT))"
      ],
      "metadata": {
        "id": "TFTYQ2Z_vJPT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DihwIF6Pvihl",
        "outputId": "8fca1775-c15b-458f-d735-3975dcbac60b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.19.5)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.10.0+cu111)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchtext) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.metrics import bleu_score"
      ],
      "metadata": {
        "id": "UA7cOs4Ev-88"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy import data"
      ],
      "metadata": {
        "id": "rN3UC86ewBu8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('questions_small.txt') as f:\n",
        "    question_snt = list(map(str.strip, f.readlines()))\n",
        "    \n",
        "with open('answers_small.txt') as f:\n",
        "    answer_snt = list(map(str.strip, f.readlines()))\n",
        "    \n",
        "examples = [data.Example.fromlist(x, fields) for x in tqdm(zip(question_snt, answer_snt),total=len(answer_snt))]\n",
        "test = data.Dataset(examples[-1000:], fields)\n",
        "train, valid = data.Dataset(examples[:-1000], fields).split(0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mrNZBobvBps",
        "outputId": "4df4492e-f735-4d41-8bee-a2a9eb4a794e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/99999 [02:17<?, ?it/s]\n",
            "  0%|          | 0/99999 [02:07<?, ?it/s]\n",
            "\n",
            "\n",
            "  1%|          | 731/99999 [00:00<00:17, 5603.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "  2%|▏         | 1787/99999 [00:00<00:11, 8192.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "  3%|▎         | 2901/99999 [00:00<00:10, 9452.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "  4%|▍         | 3937/99999 [00:00<00:09, 9793.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "  5%|▌         | 5014/99999 [00:00<00:09, 10135.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "  6%|▌         | 6069/99999 [00:00<00:09, 10270.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "  7%|▋         | 7105/99999 [00:00<00:15, 6014.16it/s] \u001b[A\u001b[A\n",
            "\n",
            "  8%|▊         | 8219/99999 [00:01<00:12, 7101.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "  9%|▉         | 9230/99999 [00:01<00:11, 7800.55it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10%|█         | 10330/99999 [00:01<00:10, 8596.87it/s]\u001b[A\u001b[A\n",
            "\n",
            " 11%|█▏        | 11317/99999 [00:01<00:09, 8921.16it/s]\u001b[A\u001b[A\n",
            "\n",
            " 12%|█▏        | 12469/99999 [00:01<00:09, 9627.62it/s]\u001b[A\u001b[A\n",
            "\n",
            " 14%|█▎        | 13628/99999 [00:01<00:08, 10175.72it/s]\u001b[A\u001b[A\n",
            "\n",
            " 15%|█▍        | 14731/99999 [00:01<00:08, 10418.47it/s]\u001b[A\u001b[A\n",
            "\n",
            " 16%|█▌        | 15904/99999 [00:01<00:07, 10798.31it/s]\u001b[A\u001b[A\n",
            "\n",
            " 17%|█▋        | 17039/99999 [00:01<00:07, 10958.76it/s]\u001b[A\u001b[A\n",
            "\n",
            " 18%|█▊        | 18156/99999 [00:01<00:07, 10971.64it/s]\u001b[A\u001b[A\n",
            "\n",
            " 19%|█▉        | 19268/99999 [00:02<00:07, 10995.91it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 20378/99999 [00:02<00:07, 10551.59it/s]\u001b[A\u001b[A\n",
            "\n",
            " 21%|██▏       | 21444/99999 [00:02<00:07, 10435.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 23%|██▎       | 22515/99999 [00:02<00:07, 10512.91it/s]\u001b[A\u001b[A\n",
            "\n",
            " 24%|██▎       | 23612/99999 [00:02<00:07, 10644.59it/s]\u001b[A\u001b[A\n",
            "\n",
            " 25%|██▍       | 24711/99999 [00:02<00:07, 10744.93it/s]\u001b[A\u001b[A\n",
            "\n",
            " 26%|██▌       | 25854/99999 [00:02<00:06, 10945.77it/s]\u001b[A\u001b[A\n",
            "\n",
            " 27%|██▋       | 26952/99999 [00:02<00:06, 10902.25it/s]\u001b[A\u001b[A\n",
            "\n",
            " 28%|██▊       | 28093/99999 [00:02<00:06, 11051.80it/s]\u001b[A\u001b[A\n",
            "\n",
            " 29%|██▉       | 29200/99999 [00:02<00:06, 10990.35it/s]\u001b[A\u001b[A\n",
            "\n",
            " 30%|███       | 30339/99999 [00:03<00:06, 11108.37it/s]\u001b[A\u001b[A\n",
            "\n",
            " 31%|███▏      | 31451/99999 [00:03<00:06, 10573.53it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 32515/99999 [00:03<00:06, 10434.66it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34%|███▎      | 33563/99999 [00:03<00:10, 6110.14it/s] \u001b[A\u001b[A\n",
            "\n",
            " 35%|███▍      | 34630/99999 [00:03<00:09, 6991.31it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36%|███▌      | 35695/99999 [00:03<00:08, 7785.16it/s]\u001b[A\u001b[A\n",
            "\n",
            " 37%|███▋      | 36724/99999 [00:03<00:07, 8376.31it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38%|███▊      | 37843/99999 [00:04<00:06, 9085.76it/s]\u001b[A\u001b[A\n",
            "\n",
            " 39%|███▉      | 39014/99999 [00:04<00:06, 9781.43it/s]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 40076/99999 [00:04<00:06, 9676.99it/s]\u001b[A\u001b[A\n",
            "\n",
            " 41%|████      | 41220/99999 [00:04<00:05, 10160.44it/s]\u001b[A\u001b[A\n",
            "\n",
            " 42%|████▏     | 42352/99999 [00:04<00:05, 10486.31it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|████▎     | 43536/99999 [00:04<00:05, 10873.79it/s]\u001b[A\u001b[A\n",
            "\n",
            " 45%|████▍     | 44649/99999 [00:04<00:05, 10768.53it/s]\u001b[A\u001b[A\n",
            "\n",
            " 46%|████▌     | 45744/99999 [00:04<00:05, 10805.43it/s]\u001b[A\u001b[A\n",
            "\n",
            " 47%|████▋     | 46848/99999 [00:04<00:04, 10873.03it/s]\u001b[A\u001b[A\n",
            "\n",
            " 48%|████▊     | 48011/99999 [00:04<00:04, 11094.42it/s]\u001b[A\u001b[A\n",
            "\n",
            " 49%|████▉     | 49128/99999 [00:05<00:04, 11006.25it/s]\u001b[A\u001b[A\n",
            "\n",
            " 50%|█████     | 50237/99999 [00:05<00:04, 11029.25it/s]\u001b[A\u001b[A\n",
            "\n",
            " 51%|█████▏    | 51344/99999 [00:05<00:04, 10836.51it/s]\u001b[A\u001b[A\n",
            "\n",
            " 52%|█████▏    | 52431/99999 [00:05<00:04, 10569.28it/s]\u001b[A\u001b[A\n",
            "\n",
            " 54%|█████▎    | 53532/99999 [00:05<00:04, 10695.89it/s]\u001b[A\u001b[A\n",
            "\n",
            " 55%|█████▍    | 54696/99999 [00:05<00:04, 10970.46it/s]\u001b[A\u001b[A\n",
            "\n",
            " 56%|█████▌    | 55796/99999 [00:05<00:04, 10784.60it/s]\u001b[A\u001b[A\n",
            "\n",
            " 57%|█████▋    | 56975/99999 [00:05<00:03, 11076.68it/s]\u001b[A\u001b[A\n",
            "\n",
            " 58%|█████▊    | 58085/99999 [00:05<00:03, 10877.22it/s]\u001b[A\u001b[A\n",
            "\n",
            " 59%|█████▉    | 59223/99999 [00:05<00:03, 11019.53it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 60369/99999 [00:06<00:03, 11147.95it/s]\u001b[A\u001b[A\n",
            "\n",
            " 61%|██████▏   | 61486/99999 [00:06<00:03, 11070.96it/s]\u001b[A\u001b[A\n",
            "\n",
            " 63%|██████▎   | 62595/99999 [00:06<00:03, 10703.66it/s]\u001b[A\u001b[A\n",
            "\n",
            " 64%|██████▍   | 63764/99999 [00:06<00:03, 10989.00it/s]\u001b[A\u001b[A\n",
            "\n",
            " 65%|██████▍   | 64866/99999 [00:06<00:03, 10829.11it/s]\u001b[A\u001b[A\n",
            "\n",
            " 66%|██████▌   | 65952/99999 [00:06<00:03, 10311.15it/s]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 67014/99999 [00:06<00:03, 10394.42it/s]\u001b[A\u001b[A\n",
            "\n",
            " 68%|██████▊   | 68058/99999 [00:07<00:05, 5405.94it/s] \u001b[A\u001b[A\n",
            "\n",
            " 69%|██████▉   | 69144/99999 [00:07<00:04, 6366.71it/s]\u001b[A\u001b[A\n",
            "\n",
            " 70%|███████   | 70193/99999 [00:07<00:04, 7196.16it/s]\u001b[A\u001b[A\n",
            "\n",
            " 71%|███████   | 71200/99999 [00:07<00:03, 7837.36it/s]\u001b[A\u001b[A\n",
            "\n",
            " 72%|███████▏  | 72238/99999 [00:07<00:03, 8449.98it/s]\u001b[A\u001b[A\n",
            "\n",
            " 73%|███████▎  | 73326/99999 [00:07<00:02, 9073.34it/s]\u001b[A\u001b[A\n",
            "\n",
            " 74%|███████▍  | 74465/99999 [00:07<00:02, 9693.28it/s]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▌  | 75547/99999 [00:07<00:02, 10004.37it/s]\u001b[A\u001b[A\n",
            "\n",
            " 77%|███████▋  | 76613/99999 [00:07<00:02, 10189.40it/s]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 77675/99999 [00:08<00:02, 9875.50it/s] \u001b[A\u001b[A\n",
            "\n",
            " 79%|███████▉  | 78785/99999 [00:08<00:02, 10218.93it/s]\u001b[A\u001b[A\n",
            "\n",
            " 80%|███████▉  | 79832/99999 [00:08<00:02, 10025.22it/s]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████  | 80852/99999 [00:08<00:01, 9933.97it/s] \u001b[A\u001b[A\n",
            "\n",
            " 82%|████████▏ | 81858/99999 [00:08<00:01, 9847.67it/s]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 82960/99999 [00:08<00:01, 10185.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 84%|████████▍ | 84054/99999 [00:08<00:01, 10404.47it/s]\u001b[A\u001b[A\n",
            "\n",
            " 85%|████████▌ | 85101/99999 [00:08<00:01, 10403.48it/s]\u001b[A\u001b[A\n",
            "\n",
            " 86%|████████▌ | 86146/99999 [00:08<00:01, 10230.81it/s]\u001b[A\u001b[A\n",
            "\n",
            " 87%|████████▋ | 87212/99999 [00:09<00:01, 10354.88it/s]\u001b[A\u001b[A\n",
            "\n",
            " 88%|████████▊ | 88314/99999 [00:09<00:01, 10549.01it/s]\u001b[A\u001b[A\n",
            "\n",
            " 89%|████████▉ | 89381/99999 [00:09<00:01, 10584.17it/s]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 90441/99999 [00:09<00:00, 10511.52it/s]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████▏| 91494/99999 [00:09<00:00, 10103.41it/s]\u001b[A\u001b[A\n",
            "\n",
            " 93%|█████████▎| 92539/99999 [00:09<00:00, 10202.33it/s]\u001b[A\u001b[A\n",
            "\n",
            " 94%|█████████▎| 93648/99999 [00:09<00:00, 10459.76it/s]\u001b[A\u001b[A\n",
            "\n",
            " 95%|█████████▍| 94794/99999 [00:09<00:00, 10746.43it/s]\u001b[A\u001b[A\n",
            "\n",
            " 96%|█████████▌| 95965/99999 [00:09<00:00, 11028.71it/s]\u001b[A\u001b[A\n",
            "\n",
            " 97%|█████████▋| 97070/99999 [00:09<00:00, 10881.62it/s]\u001b[A\u001b[A\n",
            "\n",
            " 98%|█████████▊| 98177/99999 [00:10<00:00, 10933.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 99999/99999 [00:10<00:00, 9825.11it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train, max_size=10000, min_freq=3)"
      ],
      "metadata": {
        "id": "a35c7vHRwzaM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqeTWt6RNQpp",
        "outputId": "b7c5f157-5db8-42b2-d61f-71dfccc01d07"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.legacy.data.field.Field at 0x7fc0de514250>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train, valid, test), \n",
        "                                                                      batch_size = BATCH_SIZE, \n",
        "                                                                      sort_within_batch=True,\n",
        "                                                                      sort_key=lambda x: len(x.src),\n",
        "                                                                      device = device)"
      ],
      "metadata": {
        "id": "2WqDf73Ew1TK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_batch = next(iter(test_iterator))\n",
        "test_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-pmw9EXNzCB",
        "outputId": "fc89de3d-dcb1-44ee-b93a-381184afa27a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[torchtext.legacy.data.batch.Batch of size 32]\n",
              "\t[.src]:[torch.cuda.LongTensor of size 32x50 (GPU 0)]\n",
              "\t[.tgt]:[torch.cuda.LongTensor of size 32x50 (GPU 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "    super(EncoderLSTM, self).__init__()\n",
        "\n",
        "    # Size of the one hot vectors that will be the input to the encoder\n",
        "    #self.input_size = input_size\n",
        "\n",
        "    # Output size of the word embedding NN\n",
        "    #self.embedding_size = embedding_size\n",
        "\n",
        "    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Number of layers in the lstm\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Regularization parameter\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.tag = True\n",
        "\n",
        "    # Shape --------------------> (5376, 300) [input size, embedding dims]\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    \n",
        "    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "\n",
        "  # Shape of x (26, 32) [Sequence_length, batch_size]\n",
        "  def forward(self, x):\n",
        "\n",
        "    # Shape -----------> (26, 32, 300) [Sequence_length , batch_size , embedding dims]\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    \n",
        "    # Shape --> outputs (26, 32, 1024) [Sequence_length , batch_size , hidden_size]\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size]\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
        "\n",
        "    return hidden_state, cell_state\n",
        "\n",
        "input_size_encoder = len(TEXT.vocab)\n",
        "encoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "encoder_dropout = 0.5\n",
        "\n",
        "encoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,\n",
        "                           hidden_size, num_layers, encoder_dropout).to(device)\n",
        "print(encoder_lstm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwObUlhZw88q",
        "outputId": "1bec3447-264a-475b-e453-619b4c447eea"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EncoderLSTM(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(10004, 300)\n",
            "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size_encoder = len(TEXT.vocab)\n",
        "encoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "encoder_dropout = 0.5\n",
        "\n",
        "encoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,\n",
        "                           hidden_size, num_layers, encoder_dropout).to(device)\n",
        "print(encoder_lstm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ne3lGOcPtL4",
        "outputId": "96a75639-796b-4693-d54b-272c788355db"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EncoderLSTM(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(10004, 300)\n",
            "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_state_encoder, cell_state = encoder_lstm(test_batch.src)\n",
        "cell_state.shape, hidden_state_encoder.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EizYTztrLKuK",
        "outputId": "cf6bb6db-549e-454f-e12f-1a0fa3ccf918"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 50, 1024]), torch.Size([2, 50, 1024]))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=64):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.emb_layer = torch.nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.emb_layer(input).view(1, 1, -1)\n",
        "    \n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "      \n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "MBQNQuePLH4z"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_size = len(TEXT.vocab)\n",
        "'''attention = AttnDecoderRNN(hidden_size, output_size).to(device)\n",
        "output, hidden, attention_weight = attention(hidden_state, cell_state)\n",
        "attention_weight.shape'''\n",
        "attention = AttnDecoderRNN(hidden_size, output_size).to(device)"
      ],
      "metadata": {
        "id": "1F012moyOfHu"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n",
        "    super(DecoderLSTM, self).__init__()\n",
        "\n",
        "    # Size of the one hot vectors that will be the input to the encoder\n",
        "    #self.input_size = input_size\n",
        "\n",
        "    # Output size of the word embedding NN\n",
        "    #self.embedding_size = embedding_size\n",
        "\n",
        "    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Number of layers in the lstm\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Size of the one hot vectors that will be the output to the encoder (English Vocab Size)\n",
        "    self.output_size = output_size\n",
        "\n",
        "    # Regularization parameter\n",
        "    self.dropout = nn.Dropout(p)\n",
        "\n",
        "    # Shape --------------------> (5376, 300) [input size, embedding dims]\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "\n",
        "    # Shape -----------> (1024, 4556) [embedding dims, hidden size, num layers]\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    unk_replace: True\n",
        "\n",
        "  # Shape of x (32) [batch_size]\n",
        "  def forward(self, x, hidden_state, cell_state):\n",
        "\n",
        "    # Shape of x (1, 32) [1, batch_size]\n",
        "    x = x.unsqueeze(0)\n",
        "\n",
        "    # Shape -----------> (1, 32, 300) [1, batch_size, embedding dims]\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "    # Shape --> outputs (1, 32, 1024) [1, batch_size , hidden_size]\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n",
        "    # Shape --> predictions (1, 32, 4556) [ 1, batch_size , output_size]\n",
        "    predictions = self.fc(outputs)\n",
        "\n",
        "    # Shape --> predictions (32, 4556) [batch_size , output_size]\n",
        "    predictions = predictions.squeeze(0)\n",
        "\n",
        "    return predictions, hidden_state, cell_state\n",
        "\n"
      ],
      "metadata": {
        "id": "ojblVX_LxHcM"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size_decoder = len(TEXT.vocab)\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "decoder_dropout = 0.5\n",
        "output_size = len(TEXT.vocab)\n",
        "\n",
        "decoder_lstm = DecoderLSTM(input_size_decoder, decoder_embedding_size,\n",
        "                           hidden_size, num_layers, decoder_dropout, output_size).to(device)\n",
        "print(decoder_lstm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg61XyuQQHtY",
        "outputId": "d1c7a03f-c983-4577-8959-0a130ed27a90"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecoderLSTM(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(10004, 300)\n",
            "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            "  (fc): Linear(in_features=1024, out_features=10004, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''attention_scores = torch.bmm(decoder_hidden, encoder_hidden.transpose(1, 2))\n",
        "# заметим, что у нас добавилась одна размерность и поэтому чуть меняем софтмакc\n",
        "attention_distribution = torch.softmax(attention_scores, 2)\n",
        "attention_vectors = torch.bmm(attention_distribution, encoder_hidden)\n",
        "decoder_with_attention = torch.cat([decoder_hidden, attention_vectors], dim=-1)'''\n"
      ],
      "metadata": {
        "id": "TYhN-MEOJjHI"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, Encoder_LSTM, Decoder_LSTM):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.Encoder_LSTM = Encoder_LSTM\n",
        "    self.Decoder_LSTM = Decoder_LSTM\n",
        "\n",
        "\n",
        "  def forward(self, source, target, tfr=0.5):\n",
        "    # Shape - Source : (10, 32) [(Sentence length German + some padding), Number of Sentences]\n",
        "    batch_size = source.shape[1]\n",
        "\n",
        "    # Shape - Source : (14, 32) [(Sentence length English + some padding), Number of Sentences]\n",
        "    target_len = target.shape[0]\n",
        "    target_vocab_size = len(TEXT.vocab)\n",
        "    \n",
        "    # Shape --> outputs (14, 32, 5766) \n",
        "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) ,(2, 32, 1024) [num_layers, batch_size size, hidden_size] (contains encoder's hs, cs - context vectors)\n",
        "    hidden_state_encoder, cell_state = self.Encoder_LSTM(source)\n",
        "\n",
        "    # attention_scores = torch.bmm(hidden_state_decoder, hidden_state_encoder.transpose(1, 2))\n",
        "    # заметим, что у нас добавилась одна размерность и поэтому чуть меняем софтмакc\n",
        "    # attention_distribution = torch.softmax(attention_scores, 2)\n",
        "    # attention_vectors = torch.bmm(attention_distribution, hidden_state_encoder)\n",
        "\n",
        "\n",
        "    # Shape of x (32 elements)\n",
        "    x = target[0] # Trigger token <SOS>\n",
        "\n",
        "    for i in range(1, target_len):\n",
        "      # Shape --> output (32, 5766) \n",
        "      output, hidden_state_decoder, cell_state = self.Decoder_LSTM(x, hidden_state_encoder, cell_state)\n",
        "      attention_scores = torch.bmm(hidden_state_decoder, hidden_state_encoder.transpose(1, 2))\n",
        "      attention_distribution = torch.softmax(attention_scores, 2)\n",
        "      attention_vectors = torch.bmm(attention_distribution, hidden_state_encoder)\n",
        "      outputs[i] = output\n",
        "      decoder_with_attention = torch.cat([hidden_state_decoder, attention_vectors], dim=-1)\n",
        "      best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
        "      x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
        "\n",
        "    # Shape --> outputs (14, 32, 5766) \n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "oO-ROdQSxQ6Y"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "5ZvFuSmTxnRQ"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "\n",
        "learning_rate = 0.001\n",
        "writer = SummaryWriter(f\"runs/loss_plot\")\n",
        "step = 0\n",
        "\n",
        "model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "pad_idx = TEXT.vocab.stoi['[PAD]']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "metadata": {
        "id": "qxFuAvU5xWrE"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt_ZPhXLxptU",
        "outputId": "4cb4196f-9788-4f26-efb8-2cb977ad25c4"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (Encoder_LSTM): EncoderLSTM(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(10004, 300)\n",
              "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
              "  )\n",
              "  (Decoder_LSTM): DecoderLSTM(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(10004, 300)\n",
              "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
              "    (fc): Linear(in_features=1024, out_features=10004, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(model, sentence, TEXT, device, max_length=50):\n",
        "    spacy_ger = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "    if type(sentence) == str:\n",
        "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "    tokens.insert(0, TEXT.init_token)\n",
        "    tokens.append(TEXT.eos_token)\n",
        "    text_to_indices = [TEXT.vocab.stoi[token] for token in tokens]\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Build encoder hidden, cell state\n",
        "    with torch.no_grad():\n",
        "        hidden_state_encoder, cell = model.Encoder_LSTM(sentence_tensor)\n",
        "\n",
        "    outputs = [TEXT.vocab.stoi['[BOS]']]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden_state_decoder, cell = model.Decoder_LSTM(previous_word, hidden_state_encoder, cell)\n",
        "            attention_scores = torch.bmm(hidden_state_decoder, hidden_state_encoder.transpose(1, 2))\n",
        "            attention_distribution = torch.softmax(attention_scores, 2)\n",
        "            attention_vectors = torch.bmm(attention_distribution, hidden_state_encoder)\n",
        "            decoder_with_attention = torch.cat([hidden_state_decoder, attention_vectors], dim=-1)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == TEXT.vocab.stoi['[EOS]']:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [TEXT.vocab.itos[idx] for idx in outputs]\n",
        "    return translated_sentence[1:]\n",
        "\n",
        "def checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss):\n",
        "    print('saving')\n",
        "    print()\n",
        "    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n",
        "    torch.save(state, '/content/checkpoint-NMT')\n",
        "    torch.save(model.state_dict(),'/content/checkpoint-NMT-SD')"
      ],
      "metadata": {
        "id": "CqOsUUAax5vi"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_snt[3] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "-PDJFfD9x_5K",
        "outputId": "da447e2c-2ec5-4691-e9fa-3bb0cebbc753"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'нужен ли автомобиль семье с маленьким ребенком?'"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_batch = next(iter(test_iterator))\n",
        "test_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as6Xi1niR_Wl",
        "outputId": "d9214720-285b-440f-c03c-18e14878aac8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[torchtext.legacy.data.batch.Batch of size 32]\n",
              "\t[.src]:[torch.cuda.LongTensor of size 32x50 (GPU 0)]\n",
              "\t[.tgt]:[torch.cuda.LongTensor of size 32x50 (GPU 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_loss = 0.0\n",
        "num_epochs = 100\n",
        "best_loss = 999999\n",
        "best_epoch = -1\n",
        "sentence1 = \"нужен ли автомобиль семье с маленьким ребенком?\"\n",
        "ts1  = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n",
        "  model.eval()\n",
        "  translated_sentence1 = translate_sentence(model, sentence1, TEXT, device, max_length=50)\n",
        "  print(f\"Translated example sentence 1: \\n {translated_sentence1}\")\n",
        "  ts1.append(translated_sentence1)\n",
        "\n",
        "  model.train(True)\n",
        "  for batch_idx, batch in tqdm(enumerate(train_iterator), total=len(train_iterator)):\n",
        "    input = batch.src.to(device)\n",
        "    target = batch.tgt.to(device)\n",
        "\n",
        "\n",
        "    # Pass the input and target for model's forward method\n",
        "    output = model(input, target)\n",
        "    output = output[1:].reshape(-1, output.shape[2])\n",
        "    target = target[1:].reshape(-1)\n",
        "\n",
        "    # Clear the accumulating gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate the loss value for every epoch\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    # Calculate the gradients for weights & biases using back-propagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the gradient value is it exceeds > 1\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "    # Update the weights values using the gradients we calculated using bp \n",
        "    optimizer.step()\n",
        "    step += 1\n",
        "    epoch_loss += loss.item()\n",
        "    writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
        "\n",
        "  if epoch_loss < best_loss:\n",
        "    best_loss = epoch_loss\n",
        "    best_epoch = epoch\n",
        "    checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n",
        "    if ((epoch - best_epoch) >= 10):\n",
        "      print(\"no improvement in 10 epochs, break\")\n",
        "      break\n",
        "  print(\"Epoch_Loss - {}\".format(loss.item()))\n",
        "  print()\n",
        "  \n",
        "print(epoch_loss / len(train_iterator))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE5GTL_SyFM1",
        "outputId": "21768287-af08-4f7a-d705-8d68701e471a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 1 / 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/util.py:833: UserWarning: [W095] Model 'ru_core_news_sm' (3.1.0) was trained with spaCy v3.1 and may not be 100% compatible with the current version (3.2.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated example sentence 1: \n",
            " ['кали', 'нас', 'летних', 'вконтакте', 'вконтакте', 'перестать', 'перестать', 'перестать', 'перестать', 'перестать', 'перестать', 'влади', 'влади', 'ооооо', 'ооооо', 'девочкам', 'перестать', 'перестать', 'перестать', 'перестать', 'влади', 'влади', 'ооооо', 'ооооо', 'девочкам', 'перестать', 'перестать', 'перестать', 'перестать', 'влади', 'влади', 'ооооо', 'ооооо', 'девочкам', 'перестать', 'перестать', 'перестать', 'перестать', 'влади', 'влади', 'ооооо', 'ооооо', 'девочкам', 'перестать', 'перестать', 'перестать', 'перестать', 'влади', 'влади', 'ооооо']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▊        | 519/2785 [06:29<28:24,  1.33it/s]"
          ]
        }
      ]
    }
  ]
}